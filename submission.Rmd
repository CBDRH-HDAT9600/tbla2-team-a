---
title: "HDAT9600 Team Based Learning Activity"
subtitle: "TBLA 2. Linear model"
author: "Team A/The Outliers - Melvin Galera, Peter Nguyen, Melissa Weerappah, Kevin Qlintang"
date: "25 February 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(cowplot)
library(kableExtra)

```

## Instructions

Using the `prostate` dataset (which comes with the `faraway` package for R) write some R code that

1) Performs a simple exploratory data analysis (EDA)
1) Fits a linear model, with the `lpsa` variable as the outcome (response), and the `lcavol` variable as the single predictor
1) Displays the R-squared and the residual standard error (RSE) for this model
1) Adds each of the variables `lweight`, `svi`, `lbph`, `age`, `lcp`, `pgg45` and `gleason` to the model, one-by-one
1) Displays the R-squared and RSE for each of these models
1) Creates two plots, one displaying the trend in R-squared as predictor variables are successively added to each model, and another plot showing the trend in RSE as predictor variables are successively added to each model
1) Finally, briefly describe what you observe in these plots


## Accessing the data

We can use the `data()` function from the `utils` package to load the data into the R environment, and the `head()` function to take a sneak peek.

```{r}
data(prostate, package = "faraway")
head(prostate)


```

Try entering `?faraway::prostate` at the console to get more information about this dataset. 

## Submission

Work with your group to add your analysis to this file using appropriate Rmarkdown formatting and R code chunks. Add and commit your changes as you go. Once you are ready, push all your changes to submit. If necessary, check out the introductory tutorials on knitr and git. 

<br>

## Solution

<br>

#### 1. Exploratory Data Analysis

* In this section, we perform simple exploratory data analysis on the `prostate` dataset.

a. To see the summary statistics for each variable in the `prostate` dataset:


```{r}
str(prostate)

summary(prostate)

```

* The `prostate` data set contains 97 observations and 9 variables. All the variables are numeric.


```{r include = FALSE }
# Check for missing values

sum(is.na(prostate))

# Check variables with integer data type - svi, gleason and pgg45
unique(prostate$svi)
unique(prostate$gleason)
unique(prostate$pgg45)

# Convert svi and gleason to factors
prostate$svi <- as.factor(prostate$svi)
prostate$gleason <- as.factor(prostate$gleason)

summary(prostate)

```

* There are no missing values in the data set.

<br>

##### _Univariate Summaries_

<br>

```{r echo = FALSE, fig.align='center', fig.width=10, fig.height=8, message = FALSE}

lpsa_plot <- ggplot(prostate, aes(x = lpsa)) + 
            geom_histogram(fill = 'salmon2') +
            labs(title = "lpsa distribution")
  
lcavol_plot <- ggplot(prostate, aes(x = lcavol)) + 
            geom_histogram(fill = 'salmon2') +
            labs(title = "lcavol distribution")

lweight_plot <- ggplot(prostate, aes(x = lweight)) + 
            geom_histogram(fill = 'salmon2') +
            labs(title = "lweight distribution")

lbph_plot <- ggplot(prostate, aes(x = lbph)) + 
            geom_histogram(fill = 'salmon2') +
            labs(title = "lweight distribution")

age_plot <- ggplot(prostate, aes(x = age)) + 
            geom_histogram(fill = 'salmon2') +
            labs(title = "age distribution")

lcp_plot <- ggplot(prostate, aes(x = lcp)) + 
            geom_histogram(fill = 'salmon2') +
            labs(title = "lcp distribution")

pgg45_plot <- ggplot(prostate, aes(x = pgg45)) + 
            geom_histogram(fill = 'salmon2') +
            labs(title = "pgg45 distribution")

svi_plot <- ggplot(prostate, aes(x = svi)) +
            geom_bar(fill = 'salmon2', width = 0.5) +
            labs(title = "svi distribution") 

gleason_plot <- ggplot(prostate, aes(x = gleason)) +
            geom_bar(fill = 'salmon2', width = 0.5) +
            labs(title = "gleason distribution") 


plot_grid(lpsa_plot, lcavol_plot, lweight_plot, 
          lbph_plot, age_plot, lcp_plot, 
          pgg45_plot, svi_plot, gleason_plot, nrow = 3, ncol = 3)
```

<br>

##### _Bivariate Summaries_ 

```{r echo = FALSE, fig.align='center', fig.width=8, fig.height=6, message = FALSE}
p1 <- ggplot(prostate, aes(lcavol, lpsa)) +
      geom_point(color = 'black', size = 2) + 
      geom_smooth(method = 'lm', se = FALSE, lwd = 0.5)

p2 <- ggplot(prostate, aes(lweight, lpsa)) +
      geom_point(color = 'green4', size = 2)+ 
      geom_smooth(method = 'lm', se = FALSE, lwd = 0.5)

p3 <- ggplot(prostate, aes(lbph, lpsa)) +
      geom_point(color = 'coral1', size = 2)+ 
      geom_smooth(method = 'lm', se = FALSE, lwd = 0.5)

p4 <- ggplot(prostate, aes(age, lpsa)) +
      geom_point(color = 'red3', size = 2)+ 
      geom_smooth(method = 'lm', se = FALSE, lwd = 0.5)

p4 <- ggplot(prostate, aes(age, lpsa)) +
      geom_point(color = 'red3', size = 2)+ 
      geom_smooth(method = 'lm', se = FALSE, lwd = 0.5)

p5 <- ggplot(prostate, aes(lcp, lpsa)) +
      geom_point(color = 'goldenrod3', size = 2)+ 
      geom_smooth(method = 'lm', se = FALSE, lwd = 0.5)

p6 <- ggplot(prostate, aes(pgg45, lpsa)) +
      geom_point(color = 'orchid4', size = 2)+ 
      geom_smooth(method = 'lm', se = FALSE, lwd = 0.5)

p7 <- ggplot(prostate, aes(svi, lpsa, fill = svi)) +
      geom_boxplot(alpha = 0.3) +
      theme(legend.position = "none")

p8 <- ggplot(prostate, aes(gleason, lpsa, fill = gleason)) +
      geom_boxplot(alpha = 0.3) +
      theme(legend.position = "none")

plot_grid(p1, p2, p3, p4, 
          labels = c('lpsa vs. lcavol', 'lpsa vs. lweight', 'lpsa vs. lbph', 'lpsa vs. age'),
          label_size = 12, 
          nrow = 2, ncol = 2)

plot_grid(p5, p6, p7, p8, 
          labels = c('lpsa vs. lcp', 'lpsa vs. pgg45', 'lpsa vs. svi', 'lpsa vs. gleason'),
          label_size = 12, 
          nrow = 2, ncol = 2)

```


***
<br>

#### 2. Fit a linear model, with the `lpsa` variable as the outcome (response), and the `lcavol` variable as the single predictor

* The linear model equation will be: 
<br/>


<div align = 'center'> _lpsa = $\beta_{0}$ + $\beta_{1}$lcavol + $\epsilon$_ </div>  
<br/>


* The R code to fit the model via OLS can be written as:

```{r }
mod1 <- lm(lpsa ~ lcavol, data = prostate)

```

***
<br> 

#### 3. Display the R-squared and the residual standard error (RSE) for this model


```{r}

mod1_R2 <- summary(mod1)$r.squared

mod1_RSE <- summary(mod1)$sigma

```

```{r echo = FALSE}
# create a simple dataframe to display the R-squared RSE values for the linear model

modelnum <- c('LM 2', 'LM 3', 'LM 4', 'LM 5', 'LM 6', 'LM 7', 'LM 8')

pred_vars <- c('lcavol + lweight',
               'lcavol + lweight + svi',
               'lcavol + lweight + svi + lbph',
               'lcavol + lweight + svi + lbph + age',
               'lcavol + lweight + svi + lbph + age + lcp', 
               'lcavol + lweight + svi + lbph + age + lcp + pgg45',
               'lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason')


r2_RSE_mod1 <- data.frame("Linear Model" = 'LM 1',
                           "Predictor Variable" = 'lcavol',
                           "R-Squared" = mod1_R2,
                           "RSE" = mod1_RSE)

# use kableExtra to display the dataframe

kbl(r2_RSE_mod1, caption = "Linear Model (LM) with R-squared and Residual Standard Error (RSE) values") %>% kable_styling()
```

***
<br>

#### 4. Add each of the variables `lweight`, `svi`, `lbph`, `age`, `lcp`, `pgg45` and `gleason` to the model, one-by-one. 

* The linear model equations will be: 
<br/>

* _lpsa = $\beta_{0}$ + $\beta_{1}$lcavol + $\beta_{2}$lweight + $\epsilon$_ 
<br/>

* _lpsa = $\beta_{0}$ + $\beta_{1}$lcavol + $\beta_{2}$lweight + $\beta_{3}$svi + $\epsilon$_ 
<br/>

* _lpsa = $\beta_{0}$ + $\beta_{1}$lcavol + $\beta_{2}$lweight + $\beta_{3}$svi + $\beta_{4}$lbph + $\epsilon$_
<br/>

* _lpsa = $\beta_{0}$ + $\beta_{1}$lcavol + $\beta_{2}$lweight + $\beta_{3}$svi + $\beta_{4}$lbph + $\beta_{5}$age + $\epsilon$_
<br/>

* _lpsa = $\beta_{0}$ + $\beta_{1}$lcavol + $\beta_{2}$lweight + $\beta_{3}$svi + $\beta_{4}$lbph + $\beta_{5}$age + $\beta_{6}$lcp + $\epsilon$_
<br/>

* _lpsa = $\beta_{0}$ + $\beta_{1}$lcavol + $\beta_{2}$lweight + $\beta_{3}$svi + $\beta_{4}$lbph + $\beta_{5}$age + $\beta_{6}$lcp + $\beta_{7}$pgg45 + $\epsilon$_  

* _lpsa = $\beta_{0}$ + $\beta_{1}$lcavol + $\beta_{2}$lweight + $\beta_{3}$svi + $\beta_{4}$lbph + $\beta_{5}$age + $\beta_{6}$lcp + $\beta_{7}$pgg45 + $\beta_{8}$gleason + $\epsilon$_

<br/>


* The R code to fit the models via OLS can be written as a list. 

```{r}

#create a list of models
mod_list <- list(
    mod2 <- lm(lpsa ~ lcavol + lweight, data = prostate),
    mod3 <- lm(lpsa ~ lcavol + lweight + svi, data = prostate),
    mod4 <- lm(lpsa ~ lcavol + lweight + svi + lbph, data = prostate),
    mod5 <- lm(lpsa ~ lcavol + lweight + svi + lbph + age, data = prostate),
    mod6 <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp, data = prostate),
    mod7 <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45, data = prostate), 
    mod8 <- lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason, data = prostate)
   )
```

***
<br>

#### 5. Displays the R-squared and RSE for each of these models

* We can use create functions to obtain the adjusted R-squared values and the residual standard errors (RSE) from the `mod_list`.
Note that we are extracting the adjusted R-squared because we are building linear regression on multiple variables.

```{r}

# create functions to extract adjusted R-squared and RSE values from the list
get_R2 <- function (x) {
  summary(x)$adj.r.squared
}

get_RSE <- function (x) {
  summary(x)$sigma
}

# use sapply function to get lists of R-squared and RSE values 
r2_predictors <- sapply(mod_list, get_R2)
RSE_predictors <- sapply(mod_list, get_RSE)
```

```{r echo = FALSE}

# create a dataframe to display the R-squared RSE values for each linear model

modelnum <- c('LM 2', 'LM 3', 'LM 4', 'LM 5', 'LM 6', 'LM 7', 'LM 8')

pred_vars <- c('lcavol + lweight',
               'lcavol + lweight + svi',
               'lcavol + lweight + svi + lbph',
               'lcavol + lweight + svi + lbph + age',
               'lcavol + lweight + svi + lbph + age + lcp', 
               'lcavol + lweight + svi + lbph + age + lcp + pgg45',
               'lcavol + lweight + svi + lbph + age + lcp + pgg45 + gleason')


r2_RSE_pred_vars <- data.frame("Linear Model" = modelnum,
                           "Predictor Variables" = pred_vars,
                           "R-Squared" = r2_predictors,
                           "RSE" = RSE_predictors)

# use kableExtra to display the dataframe

kbl(r2_RSE_pred_vars, caption = "Linear Models (LM) with their R-squared (adjusted) and Residual Standard Errors (RSE) values") %>% kable_styling()

```

***
<br>

#### 6. Create two plots, one displaying the trend in R-squared as predictor variables are successively added to each model, and another plot showing the trend in RSE as predictor variables are successively added to each model


```{r include = FALSE}
# combine the dataframes created in from Tasks 2 and 4:

df_mod <- data.frame(linear_model = append('LM 1', modelnum),
                     Predictor_variables = append('lcavil', pred_vars),
                     R_squared = append(mod1_R2, r2_predictors),
                     RSE = append(mod1_RSE,RSE_predictors))
df_mod

```

<br>

##### _Plot 1: R-squared vs Linear Model_s_

<br>

```{r echo = FALSE, fig.align='center'}

ggplot(data = df_mod, aes(x = linear_model, y = R_squared)) +
  geom_point(color = "darkred", size = 2, shape = 4, stroke = 2) + 
  labs(title = "R-squared values at different linear models (Prostate dataset)",
       y = "R-squared values",
       x = "Linear Models (see tables in tasks 3 & 5)",
       caption = "Note: Used adjusted R-squared for LM 2 to LM 8") +
  ylim(0.5, 0.7)


```

<br>

##### _Plot 2: RSE vs Linear Models_

<br> 

```{r echo = FALSE, fig.align='center'}


ggplot(data = df_mod, aes(x = linear_model, y = RSE)) +
  geom_point(color = "navyblue", size = 2, shape = 4, stroke = 2) + 
  labs(title = "Residual Standard Errors (RSE) at different linear models (Prostate dataset)",
       y = "Residual Standard Errors (RSE)",
       x = "Linear Models (see tables in tasks 3 & 5)") +
  ylim(0.6, 0.85)


```

***
<br>

#### 7. Briefly describe observations in the plots

##### _Plot 1: R-squared vs Linear Model_s_



<br>

##### _Plot 1: R-squared vs Linear Model_s_




